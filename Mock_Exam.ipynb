{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPdO5y13HJoKM+XF5SCubhH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YUAN2001921/mock-exam-SIYUAN-HU/blob/main/Mock_Exam.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Section 1: Iterative Prompt Engineering (30 marks)**\n",
        "\n",
        "**1.1 Initial Prompt & Pseudocode** (8 marks)\n",
        "\n",
        "**My Initial Prompt:**\n",
        "I need to design a command-line quiz program for my Python fundamentals unit. The program should load question and answer pairs from a text file, let the user choose how many questions to attempt, randomly pick that number of unique questions, ask each question in the terminal, and then show the final score. It must use only techniques covered up to Week 10, such as basic file reading and input validation from Week 6, list handling from Week 2, and simple random selection from Week 7. Please provide clear pseudocode following the six-step planning process taught in Week 2: understanding the problem, defining inputs and outputs, working through an example, writing pseudocode, implementing, and testing.\n",
        "\n",
        "**AI’s Pseudocode Response：**"
      ],
      "metadata": {
        "id": "I51D8f19sp-b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PROBLEM RESTATEMENT:\n",
        "# Build a terminal-based quiz tool that reads Q/A pairs from a text file,\n",
        "# asks the user how many questions to attempt (N), selects N unique items at random,\n",
        "# conducts the quiz, and reports the final score.\n",
        "\n",
        "INPUTS / OUTPUTS:\n",
        "# Inputs : file path (string), N (positive integer), user answers (strings)\n",
        "# Outputs: per-question prompts and feedback, final summary \"You scored X out of N\"\n",
        "\n",
        "EXAMPLE WALKTHROUGH:\n",
        "# Suppose the file lines are:\n",
        "#   \"3+4=?,7\"\n",
        "#   \"Capital of Australia?,Canberra\"\n",
        "#   \"Python list index starts at?,0\"\n",
        "# If N=2 and user answers \"7\" then \"0\", the result is 2/2.\n",
        "\n",
        "PSEUDOCODE (course constraints only: Week 6 file + validation, Week 2 lists, Week 7 simple random):\n",
        "\n",
        "FUNCTION load_questions(file_path):\n",
        "    # Week 6: basic file reading with minimal error handling\n",
        "    INIT items = []                       # Week 2: list\n",
        "    TRY:\n",
        "        OPEN file_path FOR reading -> fh\n",
        "        FOR each line IN fh:\n",
        "            SET line = TRIM(line)\n",
        "            IF line == \"\":\n",
        "                CONTINUE\n",
        "            SPLIT line by first comma -> parts\n",
        "            IF LENGTH(parts) >= 2:\n",
        "                SET q = TRIM(parts[0])\n",
        "                SET a = TRIM(parts[1])\n",
        "                APPEND (q, a) TO items\n",
        "        CLOSE fh\n",
        "    ON ANY FILE ERROR:\n",
        "        RETURN []                         # fail gracefully\n",
        "    RETURN items\n",
        "\n",
        "FUNCTION request_valid_count(max_allowed):\n",
        "    # Week 6: basic user input validation\n",
        "    LOOP:\n",
        "        PROMPT \"How many questions? \" -> raw\n",
        "        IF raw is not an integer string:\n",
        "            PRINT \"Please enter a positive integer.\"\n",
        "            CONTINUE\n",
        "        SET n = TO_INTEGER(raw)\n",
        "        IF n <= 0 OR n > max_allowed:\n",
        "            PRINT \"Enter a number between 1 and \" + max_allowed\n",
        "            CONTINUE\n",
        "        RETURN n\n",
        "\n",
        "FUNCTION choose_unique_indices(total, n):\n",
        "    # Week 7: simple random selection without duplicates\n",
        "    MAKE idx_list = [0, 1, 2, ..., total-1]\n",
        "    SHUFFLE idx_list                       # simple random\n",
        "    RETURN first n elements of idx_list\n",
        "\n",
        "FUNCTION administer_quiz(items, indices):\n",
        "    # Week 2 lists + Week 5 loops; simple case-insensitive compare\n",
        "    SET score = 0\n",
        "    FOR each i IN indices:\n",
        "        (q, a) = items[i]\n",
        "        PRINT \"Q: \" + q\n",
        "        PROMPT \"Your answer: \" -> ans\n",
        "        IF LOWER(TRIM(ans)) == LOWER(TRIM(a)):\n",
        "            PRINT \"Correct!\"\n",
        "            score = score + 1\n",
        "        ELSE:\n",
        "            PRINT \"Incorrect. Correct answer: \" + a\n",
        "        PRINT \"-----\"\n",
        "    RETURN score\n",
        "\n",
        "MAIN PROGRAM:\n",
        "    PRINT \"=== CLI Quiz Generator ===\"\n",
        "    PROMPT \"Enter path to question file: \" -> path\n",
        "    SET items = load_questions(path)\n",
        "    IF LENGTH(items) == 0:\n",
        "        PRINT \"No questions loaded or file error. Exiting.\"\n",
        "        EXIT\n",
        "    SET n = request_valid_count(LENGTH(items))\n",
        "    SET indices = choose_unique_indices(LENGTH(items), n)\n",
        "    SET result = administer_quiz(items, indices)\n",
        "    PRINT \"You scored \" + result + \" out of \" + n\n"
      ],
      "metadata": {
        "id": "PA0ZEO4utXnD",
        "outputId": "56a52dad-2bb0-4c91-8c3c-07915af95517",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (ipython-input-1321073814.py, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-1321073814.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    PROBLEM RESTATEMENT:\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Course Reference**：\n",
        "I used the 6-step problem-solving methodology from Week 2: Computational Thinking & Algorithm Design."
      ],
      "metadata": {
        "id": "Mj2k0wowt_5f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Section 1.2 – Two Prompt Refinements (12 marks)**\n",
        "\n",
        "**First Refinement:**\n",
        "\n",
        "Please refine the pseudocode to handle file validation and user input errors. Add specific error handling for when files don't exist, when the file format is incorrect, and when users enter invalid numbers for question count.\n",
        "\n",
        "**AI’s Updated Pseudocode:**\n",
        "[Previous pseudocode enhanced with:]"
      ],
      "metadata": {
        "id": "DXglQIQpuIsE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "FUNCTION load_questions(filename):\n",
        "    # Week 6: basic file reading + simple error handling\n",
        "    IF filename does not exist:\n",
        "        DISPLAY \"Error: File not found\"\n",
        "        RETURN empty list\n",
        "\n",
        "    CREATE empty list questions\n",
        "    TRY:\n",
        "        OPEN filename FOR reading -> fh\n",
        "        FOR each line IN fh:\n",
        "            line = TRIM(line)\n",
        "            IF line == \"\": CONTINUE\n",
        "            # Expect \"question,answer\" exactly one comma\n",
        "            IF \",\" not in line OR COUNT of \",\" in line != 1:\n",
        "                DISPLAY \"Warning: Invalid format on this line, skipping\"\n",
        "                CONTINUE\n",
        "            SPLIT line by first comma -> (q, a)\n",
        "            ADD (TRIM(q), TRIM(a)) TO questions\n",
        "        CLOSE fh\n",
        "    EXCEPT any file read error:\n",
        "        DISPLAY \"Error: Could not read file\"\n",
        "        RETURN empty list\n",
        "    RETURN questions\n",
        "\n",
        "FUNCTION get_valid_number_input(prompt, max_value):\n",
        "    # Week 6: basic input validation loop\n",
        "    WHILE True:\n",
        "        DISPLAY prompt\n",
        "        raw = INPUT()\n",
        "        TRY:\n",
        "            n = TO_INTEGER(raw)\n",
        "            IF n <= 0:\n",
        "                DISPLAY \"Please enter a positive number\"\n",
        "                CONTINUE\n",
        "            IF n > max_value:\n",
        "                DISPLAY \"Using maximum available: \" + max_value\n",
        "                RETURN max_value\n",
        "            RETURN n\n",
        "        EXCEPT conversion error:\n",
        "            DISPLAY \"Please enter a valid number\"\n"
      ],
      "metadata": {
        "id": "iK-Vt73rvwP6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Course Reference**：This refinement applied Week 6 basic file operations and input validation with Week 2 lists."
      ],
      "metadata": {
        "id": "afT3BW7UxoIM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Second Refinement:**\n",
        "\n",
        "Further enhance the pseudocode to include user experience improvements: add a main menu system, option to see score statistics, and better file format handling that can detect different delimiters automatically.\n",
        "\n",
        "**AI’s Final Pseudocode:**\n",
        "[Previous pseudocode plus:]"
      ],
      "metadata": {
        "id": "LGBsBfApv2lr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "FUNCTION detect_delimiter(sample_line):\n",
        "    # Week 2: simple string handling\n",
        "    FOR delimiter IN [\"|\", \",\", \"\\t\"]:\n",
        "        IF delimiter IN sample_line AND COUNT of delimiter in sample_line == 1:\n",
        "            RETURN delimiter\n",
        "    RETURN None\n",
        "\n",
        "FUNCTION load_questions_flexible(filename):\n",
        "    # Week 6 safety + Week 2 strings\n",
        "    IF filename does not exist:\n",
        "        DISPLAY \"Error: File not found\"\n",
        "        RETURN empty list\n",
        "    CREATE questions = []\n",
        "    TRY:\n",
        "        OPEN filename FOR reading -> fh\n",
        "        # Find first non-empty line as sample\n",
        "        sample = \"\"\n",
        "        FOR line IN fh:\n",
        "            line = TRIM(line)\n",
        "            IF line != \"\":\n",
        "                sample = line\n",
        "                BREAK\n",
        "        IF sample == \"\":\n",
        "            RETURN []     # empty file\n",
        "        delim = detect_delimiter(sample)\n",
        "        IF delim IS None:\n",
        "            DISPLAY \"Error: Unsupported format\"\n",
        "            RETURN []\n",
        "        # Process sample\n",
        "        SPLIT sample by delim -> (q, a)\n",
        "        APPEND (TRIM(q), TRIM(a)) TO questions\n",
        "        # Process remaining lines\n",
        "        FOR line IN fh:\n",
        "            line = TRIM(line)\n",
        "            IF line == \"\": CONTINUE\n",
        "            IF delim NOT IN line OR COUNT of delim in line != 1:\n",
        "                DISPLAY \"Warning: Invalid format, skipping\"\n",
        "                CONTINUE\n",
        "            SPLIT line by delim -> (q, a)\n",
        "            APPEND (TRIM(q), TRIM(a)) TO questions\n",
        "        CLOSE fh\n",
        "    EXCEPT any read error:\n",
        "        DISPLAY \"Error: Could not read file\"\n",
        "        RETURN []\n",
        "    RETURN questions\n",
        "\n",
        "FUNCTION display_main_menu():\n",
        "    # Week 3: text-based menu\n",
        "    DISPLAY \"1. Take Quiz\"\n",
        "    DISPLAY \"2. View Statistics\"\n",
        "    DISPLAY \"3. Exit\"\n",
        "\n",
        "FUNCTION main_program():\n",
        "    # Week 2 lists; Week 7 random\n",
        "    scores = []\n",
        "    WHILE True:\n",
        "        CALL display_main_menu()\n",
        "        choice = INPUT(\"Enter choice: \")\n",
        "        IF choice == \"1\":\n",
        "            path = INPUT(\"Enter question file path: \")\n",
        "            qs = load_questions_flexible(path)\n",
        "            IF LENGTH(qs) == 0:\n",
        "                DISPLAY \"No valid questions available.\"\n",
        "                CONTINUE\n",
        "            n = get_valid_number_input(\"How many questions? \", LENGTH(qs))\n",
        "            idx = choose_unique_indices(LENGTH(qs), n)   # Week 7 simple random\n",
        "            result = administer_quiz(qs, idx)            # case-insensitive compare\n",
        "            APPEND result TO scores\n",
        "            DISPLAY \"You scored \" + result + \" out of \" + n\n",
        "        ELIF choice == \"2\":\n",
        "            IF LENGTH(scores) == 0:\n",
        "                DISPLAY \"No scores recorded yet.\"\n",
        "            ELSE:\n",
        "                attempts = LENGTH(scores)\n",
        "                best = MAX(scores)\n",
        "                last = scores[attempts - 1]\n",
        "                DISPLAY \"Attempts: \" + attempts\n",
        "                DISPLAY \"Best score: \" + best\n",
        "                DISPLAY \"Last score: \" + last\n",
        "        ELIF choice == \"3\":\n",
        "            DISPLAY \"Goodbye!\"\n",
        "            EXIT\n",
        "        ELSE:\n",
        "            DISPLAY \"Invalid choice\"\n"
      ],
      "metadata": {
        "id": "iWI9B4F8wpiQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Course Reference：** This refinement applied Week 3 text-based menu design, Week 7 simple random selection, and Week 2 list/string handling."
      ],
      "metadata": {
        "id": "uDpi2EacwtWV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.3 Critical Analysis with Course References (10 marks)**\n",
        "\n",
        "**Critical Analysis：**\n",
        "The two refinements systematically strengthened the design while staying within course scope. Guided by the Week 2 six-step planning method, I first addressed robustness: using Week 6 techniques, the pseudocode now validates the file path, skips malformed lines, and bounds the question count with simple input checks, preventing common failures without adding advanced constructs. Next, I improved fairness and UX by applying Week 7’s simple non-repeating random selection (shuffle indices, take first N) and Week 2 string/list handling to normalise answers (trim + lower). I also introduced a Week 3 text-based menu to structure choices and provide optional score viewing. Together, these changes yield a clear, testable flow that is modular, easy to implement, and aligned with the “≤ Week 10” constraint."
      ],
      "metadata": {
        "id": "EYlRA_921mM_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Course Reference：**\n",
        "I used the Week 2 six-step planning method and applied Week 6 validation, Week 7 simple random selection, Week 2 list/string handling, and Week 3 text-based menu design."
      ],
      "metadata": {
        "id": "08-zJsSE1vrN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Section 2: Debug & Correct Practice (25 marks)**\n",
        "\n",
        "**2.1 Error Detection Practice (10 marks)**\n",
        "\n",
        "**My Initial Error Observations:**\n",
        "\n",
        "1.Inconsistent variable name: memories is defined but memory is used later (e.g.,append and loop).\n",
        "\n",
        "2.Scope/visibility confusion for the history list inside the final loop.\n",
        "\n",
        "3.Conversation logging logic omits some user inputs, so history is incomplete.\n",
        "\n",
        "**My AI Prompt:**\n",
        "\n",
        "Please review this beginner-level chatbot script and identify typical student mistakes. Focus on variable naming consistency, basic control flow, data storage for the conversation history, and simple edge cases.\n",
        "\n",
        "**AI’s Complete Error Analysis:**\n",
        "\n",
        "**1.Variable Name Mismatch (Line ~23)**\n",
        "\n",
        "Calls memory.append(...) while the list variable is declared as memories.\n",
        "\n",
        "This will raise a NameError when appending.\n",
        "\n",
        "**2.Scope/Reference Problem (Final Loop)**\n",
        "\n",
        "The closing loop iterates over memory, which is never defined; only memories exists.\n",
        "\n",
        "Attempting to print the transcript will fail or show nothing.\n",
        "\n",
        "**3.Logic Gap in History Recording**\n",
        "\n",
        "Some branches only store bot replies, not user inputs.\n",
        "\n",
        "The transcript will be partial and not reflect the full dialogue.\n",
        "\n",
        "**4.Edge Case Handling**\n",
        "\n",
        "If the user immediately types \"quit\", the history may be empty.\n",
        "\n",
        "The final display loop should guard for an empty list to avoid confusing output.\n",
        "\n",
        "**Comparison of Findings:**\n",
        "My manual pass caught the core naming/loop issues by tracing variable usage line by line. The AI added useful edge cases (e.g., empty history on early quit) and highlighted missing user-message logging. Combining both helped form a complete error picture.\n",
        "\n",
        "**Course Reference:**\n",
        "I applied the Week 1 AI-assisted debugging mindset (spotting basic mistakes) together with early-week fundamentals (variables and control flow)."
      ],
      "metadata": {
        "id": "DliQ_G7D3qhk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.2 Fix & Learn (15 marks)**\n",
        "\n",
        "**My AI Prompt:**\n",
        "\n",
        "Please produce a corrected version of the chatbot using only introductory Python techniques. Ensure variable names are consistent, both user and bot messages are recorded, and the final conversation history is displayed safely.\n",
        "\n",
        "**My Hand-Written Corrected Version:**"
      ],
      "metadata": {
        "id": "EQfJvcW24IET"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def simple_chatbot():\n",
        "    \"\"\"\n",
        "    A basic chatbot that records a conversation history.\n",
        "\n",
        "    >>> # Minimal doctest: store a single user line in a history list\n",
        "    >>> history = []\n",
        "    >>> history.append(\"You: hello\")\n",
        "    >>> len(history)\n",
        "    1\n",
        "    \"\"\"\n",
        "    conversation_history = []  # Consistent name throughout\n",
        "    print(\"Chatbot started! Type 'quit' to exit.\")\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"You: \")\n",
        "        if user_input.lower() == \"quit\":\n",
        "            print(\"Goodbye! Here's our conversation:\")\n",
        "            break\n",
        "\n",
        "        # Record the user's message first (fix: was missing in places)\n",
        "        conversation_history.append(f\"You: {user_input}\")\n",
        "\n",
        "        # Very simple rule-based replies (intro-level control flow)\n",
        "        text = user_input.lower().strip()\n",
        "        if \"hello\" in text:\n",
        "            reply = \"Hello there!\"\n",
        "        elif \"how are you\" in text:\n",
        "            reply = \"I'm doing well, thanks!\"\n",
        "        elif \"weather\" in text:\n",
        "            reply = \"I don't have weather data, sorry!\"\n",
        "        else:\n",
        "            reply = \"That's interesting—tell me more.\"\n",
        "\n",
        "        print(f\"Bot: {reply}\")\n",
        "        # Record the bot's reply with the same history list\n",
        "        conversation_history.append(f\"Bot: {reply}\")\n",
        "\n",
        "    # Safe final display (fix: handle empty or missing entries gracefully)\n",
        "    if conversation_history:\n",
        "        for message in conversation_history:\n",
        "            print(message)\n",
        "    else:\n",
        "        print(\"No conversation to display.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    import doctest\n",
        "    doctest.testmod()\n",
        "    simple_chatbot()\n"
      ],
      "metadata": {
        "id": "ldjmbFfv5B7x",
        "outputId": "cb4c503a-9f98-40c3-9959-8141bdcc443d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "line 5 of the doctest for __main__.load_quiz_questions has an option directive on a line with no example: '# qs = load_quiz_questions(\"sample_quiz.txt\")  # doctest: +SKIP'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1713445795.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mdoctest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0mdoctest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtestmod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m     \u001b[0msimple_chatbot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/doctest.py\u001b[0m in \u001b[0;36mtestmod\u001b[0;34m(m, name, globs, verbose, report, optionflags, extraglobs, raise_on_error, exclude_empty)\u001b[0m\n\u001b[1;32m   2002\u001b[0m         \u001b[0mrunner\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDocTestRunner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptionflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptionflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2003\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2004\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfinder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mglobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextraglobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextraglobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2005\u001b[0m         \u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/doctest.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(self, obj, name, module, globs, extraglobs)\u001b[0m\n\u001b[1;32m    946\u001b[0m         \u001b[0;31m# Recursively explore `obj`, extracting DocTests.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m         \u001b[0mtests\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_find\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtests\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource_lines\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m         \u001b[0;31m# Sort the tests by alpha order of names, for consistency in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m         \u001b[0;31m# verbose-mode output.  This was a feature of doctest in Pythons\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/doctest.py\u001b[0m in \u001b[0;36m_find\u001b[0;34m(self, tests, obj, name, module, source_lines, globs, seen)\u001b[0m\n\u001b[1;32m   1020\u001b[0m                 if ((self._is_routine(val) or inspect.isclass(val)) and\n\u001b[1;32m   1021\u001b[0m                     self._from_module(module, val)):\n\u001b[0;32m-> 1022\u001b[0;31m                     self._find(tests, val, valname, module, source_lines,\n\u001b[0m\u001b[1;32m   1023\u001b[0m                                globs, seen)\n\u001b[1;32m   1024\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/doctest.py\u001b[0m in \u001b[0;36m_find\u001b[0;34m(self, tests, obj, name, module, source_lines, globs, seen)\u001b[0m\n\u001b[1;32m   1008\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1009\u001b[0m         \u001b[0;31m# Find a test for this object, and add it to the list of tests.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1010\u001b[0;31m         \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource_lines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1011\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1012\u001b[0m             \u001b[0mtests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/doctest.py\u001b[0m in \u001b[0;36m_get_test\u001b[0;34m(self, obj, name, module, globs, source_lines)\u001b[0m\n\u001b[1;32m   1090\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\".pyc\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1091\u001b[0m                 \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1092\u001b[0;31m         return self._parser.get_doctest(docstring, globs, name,\n\u001b[0m\u001b[1;32m   1093\u001b[0m                                         filename, lineno)\n\u001b[1;32m   1094\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/doctest.py\u001b[0m in \u001b[0;36mget_doctest\u001b[0;34m(self, string, globs, name, filename, lineno)\u001b[0m\n\u001b[1;32m    682\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmore\u001b[0m \u001b[0minformation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m         \"\"\"\n\u001b[0;32m--> 684\u001b[0;31m         return DocTest(self.get_examples(string, name), globs,\n\u001b[0m\u001b[1;32m    685\u001b[0m                        name, filename, lineno, string)\n\u001b[1;32m    686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/doctest.py\u001b[0m in \u001b[0;36mget_examples\u001b[0;34m(self, string, name)\u001b[0m\n\u001b[1;32m    696\u001b[0m         \u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0monly\u001b[0m \u001b[0mused\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0merror\u001b[0m \u001b[0mmessages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    697\u001b[0m         \"\"\"\n\u001b[0;32m--> 698\u001b[0;31m         return [x for x in self.parse(string, name)\n\u001b[0m\u001b[1;32m    699\u001b[0m                 if isinstance(x, Example)]\n\u001b[1;32m    700\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/doctest.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, string, name)\u001b[0m\n\u001b[1;32m    658\u001b[0m             \u001b[0;31m# Extract info from the regexp match.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    659\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwant\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_msg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 660\u001b[0;31m                      \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_example\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlineno\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    661\u001b[0m             \u001b[0;31m# Create an Example, and add it to the list.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    662\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_IS_BLANK_OR_COMMENT\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/doctest.py\u001b[0m in \u001b[0;36m_parse_example\u001b[0;34m(self, m, name, lineno)\u001b[0m\n\u001b[1;32m    739\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    740\u001b[0m         \u001b[0;31m# Extract options from the source.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 741\u001b[0;31m         \u001b[0moptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_find_options\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlineno\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    742\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwant\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_msg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/doctest.py\u001b[0m in \u001b[0;36m_find_options\u001b[0;34m(self, source, name, lineno)\u001b[0m\n\u001b[1;32m    774\u001b[0m                 \u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mflag\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'+'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    775\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moptions\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_IS_BLANK_OR_COMMENT\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 776\u001b[0;31m             raise ValueError('line %r of the doctest for %s has an option '\n\u001b[0m\u001b[1;32m    777\u001b[0m                              \u001b[0;34m'directive on a line with no example: %r'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m                              (lineno, name, source))\n",
            "\u001b[0;31mValueError\u001b[0m: line 5 of the doctest for __main__.load_quiz_questions has an option directive on a line with no example: '# qs = load_quiz_questions(\"sample_quiz.txt\")  # doctest: +SKIP'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Course References:**\n",
        "\n",
        "My error-spotting approach follows Week 1’s AI-first debugging practice (finding simple mistakes). The fix relies on early-week fundamentals only (variables, conditionals, loops, strings) within the taught scope."
      ],
      "metadata": {
        "id": "bVKA3xrF5QbQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Section 3: Debug & Refine Practice (20 marks)**\n",
        "\n",
        "**3.1 Function Debug Practice (10 marks)**\n",
        "\n",
        "**My Initial Error Observations:**\n",
        "\n",
        "The function calculate_average() did not handle empty lists, causing a ZeroDivisionError. There was no validation for non-numeric items in the list.Print statements were mixed inside the function instead of returning values.\n",
        "\n",
        "**My AI Prompt:**\n",
        "Please analyse the function calculate_average() and identify errors a beginner might make when working with lists and numeric calculations. Focus on basic logic issues and invalid input handling using only techniques up to Week 10.\n",
        "\n",
        "**AI’s Complete Error Analysis:**\n",
        "\n",
        "**Division Error:** No check for empty input list → ZeroDivisionError.\n",
        "\n",
        "**Type Error:** Loop adds non-numeric items without validation.\n",
        "\n",
        "**Design Issue: **Function prints results instead of returning them (violates Week 4 function design principles).\n",
        "\n",
        "**Testing Gap:** No doctest or example values to verify behaviour.\n",
        "\n",
        "**Comparison of Findings:**\n",
        "I manually spotted the empty-list and printing issues. The AI additionally covered type validation and testing concerns. Both align with Week 4 function design and Week 5 iteration concepts.\n",
        "\n",
        "**Course Reference:**\n",
        "I applied Week 4 function debugging and Week 5 loop validation skills for error analysis."
      ],
      "metadata": {
        "id": "Ld0v51-Q5a0M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.2 Refined Function Version (10 marks)**\n",
        "\n",
        "**My AI Prompt:**\n",
        "\n",
        "1.“I want to enhance a CSV analysis function that summarises student quiz scores. How can I safely read a CSV and check for missing columns using only beginner-level Python?”\n",
        "\n",
        "2.“Show me a clear way to handle cases where the file is empty or data is not numeric, without using advanced libraries.”\n",
        "\n",
        "3.“How can I display meaningful error or warning messages for users, based on basic try/except handling taught before Week 10?”\n",
        "\n",
        "**My Corrected Function:**"
      ],
      "metadata": {
        "id": "5bpLDh87eQE4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def refined_analyze_scores(file_path):\n",
        "    \"\"\"\n",
        "    Analyse student quiz score data from a CSV file with beginner-level error handling.\n",
        "\n",
        "    >>> refined_analyze_scores(\"\")  # Empty path\n",
        "    Error: No file path provided\n",
        "    >>> refined_analyze_scores(\"unknown.csv\")  # doctest: +SKIP\n",
        "    Error: File not found\n",
        "    \"\"\"\n",
        "    import os\n",
        "    import pandas as pd\n",
        "\n",
        "    try:\n",
        "        # Week 6: check for valid input\n",
        "        if not file_path:\n",
        "            print(\"Error: No file path provided\")\n",
        "            return None\n",
        "\n",
        "        if not os.path.exists(file_path):\n",
        "            print(f\"Error: File '{file_path}' not found.\")\n",
        "            return None\n",
        "\n",
        "        # Week 7: safe CSV reading\n",
        "        try:\n",
        "            df = pd.read_csv(file_path)\n",
        "        except:\n",
        "            print(\"Error: Unable to read file. Please ensure it is a valid CSV.\")\n",
        "            return None\n",
        "\n",
        "        # Handle empty dataset\n",
        "        if df.empty:\n",
        "            print(\"Warning: The CSV file has no data.\")\n",
        "            return {'message': 'No records to process', 'total_students': 0}\n",
        "\n",
        "        # Week 7: check for required 'score' column\n",
        "        if 'score' not in df.columns:\n",
        "            print(\"Error: Missing 'score' column in the dataset.\")\n",
        "            print(f\"Available columns: {', '.join(df.columns)}\")\n",
        "            return None\n",
        "\n",
        "        # Clean invalid rows\n",
        "        df = df[pd.to_numeric(df['score'], errors='coerce').notna()]\n",
        "        if df.empty:\n",
        "            print(\"Error: No valid numeric data in 'score' column.\")\n",
        "            return None\n",
        "\n",
        "        # Compute simple statistics\n",
        "        avg_score = round(df['score'].mean(), 2)\n",
        "        total = len(df)\n",
        "        high = len(df[df['score'] >= 80])\n",
        "        low = len(df[df['score'] < 50])\n",
        "        pass_rate = round((len(df[df['score'] >= 50]) / total) * 100, 1)\n",
        "\n",
        "        results = {\n",
        "            'average_score': avg_score,\n",
        "            'total_students': total,\n",
        "            'high_performers': high,\n",
        "            'low_performers': low,\n",
        "            'pass_rate(%)': pass_rate,\n",
        "            'data_note': 'Healthy dataset' if total >= 20 else 'Small dataset'\n",
        "        }\n",
        "\n",
        "        return results\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Unexpected error while analysing scores: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    import doctest\n",
        "    doctest.testmod()\n"
      ],
      "metadata": {
        "id": "2NyJ04iUeSc0",
        "outputId": "07159496-408f-47cc-c170-211f91d2f3aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "line 5 of the doctest for __main__.load_quiz_questions has an option directive on a line with no example: '# qs = load_quiz_questions(\"sample_quiz.txt\")  # doctest: +SKIP'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2557913921.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mdoctest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m     \u001b[0mdoctest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtestmod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/lib/python3.12/doctest.py\u001b[0m in \u001b[0;36mtestmod\u001b[0;34m(m, name, globs, verbose, report, optionflags, extraglobs, raise_on_error, exclude_empty)\u001b[0m\n\u001b[1;32m   2002\u001b[0m         \u001b[0mrunner\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDocTestRunner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptionflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptionflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2003\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2004\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfinder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mglobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextraglobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextraglobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2005\u001b[0m         \u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/doctest.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(self, obj, name, module, globs, extraglobs)\u001b[0m\n\u001b[1;32m    946\u001b[0m         \u001b[0;31m# Recursively explore `obj`, extracting DocTests.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m         \u001b[0mtests\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_find\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtests\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource_lines\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m         \u001b[0;31m# Sort the tests by alpha order of names, for consistency in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m         \u001b[0;31m# verbose-mode output.  This was a feature of doctest in Pythons\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/doctest.py\u001b[0m in \u001b[0;36m_find\u001b[0;34m(self, tests, obj, name, module, source_lines, globs, seen)\u001b[0m\n\u001b[1;32m   1020\u001b[0m                 if ((self._is_routine(val) or inspect.isclass(val)) and\n\u001b[1;32m   1021\u001b[0m                     self._from_module(module, val)):\n\u001b[0;32m-> 1022\u001b[0;31m                     self._find(tests, val, valname, module, source_lines,\n\u001b[0m\u001b[1;32m   1023\u001b[0m                                globs, seen)\n\u001b[1;32m   1024\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/doctest.py\u001b[0m in \u001b[0;36m_find\u001b[0;34m(self, tests, obj, name, module, source_lines, globs, seen)\u001b[0m\n\u001b[1;32m   1008\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1009\u001b[0m         \u001b[0;31m# Find a test for this object, and add it to the list of tests.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1010\u001b[0;31m         \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource_lines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1011\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1012\u001b[0m             \u001b[0mtests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/doctest.py\u001b[0m in \u001b[0;36m_get_test\u001b[0;34m(self, obj, name, module, globs, source_lines)\u001b[0m\n\u001b[1;32m   1090\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\".pyc\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1091\u001b[0m                 \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1092\u001b[0;31m         return self._parser.get_doctest(docstring, globs, name,\n\u001b[0m\u001b[1;32m   1093\u001b[0m                                         filename, lineno)\n\u001b[1;32m   1094\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/doctest.py\u001b[0m in \u001b[0;36mget_doctest\u001b[0;34m(self, string, globs, name, filename, lineno)\u001b[0m\n\u001b[1;32m    682\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmore\u001b[0m \u001b[0minformation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m         \"\"\"\n\u001b[0;32m--> 684\u001b[0;31m         return DocTest(self.get_examples(string, name), globs,\n\u001b[0m\u001b[1;32m    685\u001b[0m                        name, filename, lineno, string)\n\u001b[1;32m    686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/doctest.py\u001b[0m in \u001b[0;36mget_examples\u001b[0;34m(self, string, name)\u001b[0m\n\u001b[1;32m    696\u001b[0m         \u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0monly\u001b[0m \u001b[0mused\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0merror\u001b[0m \u001b[0mmessages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    697\u001b[0m         \"\"\"\n\u001b[0;32m--> 698\u001b[0;31m         return [x for x in self.parse(string, name)\n\u001b[0m\u001b[1;32m    699\u001b[0m                 if isinstance(x, Example)]\n\u001b[1;32m    700\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/doctest.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, string, name)\u001b[0m\n\u001b[1;32m    658\u001b[0m             \u001b[0;31m# Extract info from the regexp match.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    659\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwant\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_msg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 660\u001b[0;31m                      \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_example\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlineno\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    661\u001b[0m             \u001b[0;31m# Create an Example, and add it to the list.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    662\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_IS_BLANK_OR_COMMENT\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/doctest.py\u001b[0m in \u001b[0;36m_parse_example\u001b[0;34m(self, m, name, lineno)\u001b[0m\n\u001b[1;32m    739\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    740\u001b[0m         \u001b[0;31m# Extract options from the source.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 741\u001b[0;31m         \u001b[0moptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_find_options\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlineno\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    742\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwant\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_msg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/doctest.py\u001b[0m in \u001b[0;36m_find_options\u001b[0;34m(self, source, name, lineno)\u001b[0m\n\u001b[1;32m    774\u001b[0m                 \u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mflag\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'+'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    775\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moptions\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_IS_BLANK_OR_COMMENT\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 776\u001b[0;31m             raise ValueError('line %r of the doctest for %s has an option '\n\u001b[0m\u001b[1;32m    777\u001b[0m                              \u001b[0;34m'directive on a line with no example: %r'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m                              (lineno, name, source))\n",
            "\u001b[0;31mValueError\u001b[0m: line 5 of the doctest for __main__.load_quiz_questions has an option directive on a line with no example: '# qs = load_quiz_questions(\"sample_quiz.txt\")  # doctest: +SKIP'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Course Reference**\n",
        "\n",
        "In developing this function, we applied a combination of concepts progressively introduced from Week 6 to Week 8. In Week 6: File Operations and Input Validation, we learned how to verify file paths, handle missing files gracefully, and prevent the program from crashing when a user provides invalid input. Building on this, Week 7: Iteration and Data Processing taught us how to clean datasets using loops, validate column presence, and apply simple numeric operations to calculate statistics such as averages or pass rates. Finally, in Week 8: Exception Handling Fundamentals, we learned to use layered try/except structures to separate predictable file-related issues from unexpected runtime errors, allowing for user-friendly feedback and smoother program execution. Through combining these weekly topics, we demonstrated an integrated understanding of safe file handling, structured data validation, and basic error management within the Week 10 course boundaries."
      ],
      "metadata": {
        "id": "mh0TTBAUidIc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.3 Comparison Practice (4 marks)**\n",
        "**2 Similarities:**\n",
        "\n",
        "Both the original and refined versions apply basic file checks before loading data and confirm that the required 'score' column exists.\n",
        "\n",
        "Both clean non-numeric data in the 'score' field and use simple descriptive statistics such as averages and pass rates to summarise results.\n",
        "\n",
        "**2 Differences:**\n",
        "\n",
        "The improved version separates predictable file errors from unexpected ones by adding layered try/except blocks, while the earlier version only relied on a single catch-all exception.\n",
        "\n",
        "The refined code includes structured return dictionaries and user-friendly feedback, whereas the initial one mainly printed messages and lacked reusable outputs for testing.\n",
        "\n",
        "**1 Course Connection:**\n",
        "Our version reflects what we learned in Week 6 (File Operations and Validation), Week 7 (Iteration and Data Processing), and Week 8 (Exception Handling Fundamentals). We apply file-existence checking and basic input validation from Week 6, use iterative cleaning and simple aggregation from Week 7, and implement layered error handling patterns from Week 8 to enhance robustness while staying within the Week 10 scope.\n",
        "\n",
        "**1 Learning Goal:**\n",
        "Through this comparison, we recognise that future data-processing tasks will require more structured validation and detailed reporting. We aim to improve our ability to design reusable analysis functions with clearer error messages and maintainable code architecture in upcoming project-based assessments."
      ],
      "metadata": {
        "id": "jd-nVh6sivE6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Section 4: Manual Implementation & Reflection Practice (25 marks)**\n",
        "\n",
        "**Section 4.1 Independent Coding Practice (15 marks)**\n",
        "\n",
        "**AI Syntax Help Used:**\n",
        "**Prompt:** “Remind me of beginner-level syntax to read a text file line by line in Python and split each line by a '|' delimiter; also how to shuffle indices and take the first k items without using advanced libraries.”\n",
        "\n",
        "**AI Response:** “Use with open(path, 'r') as f: then iterate for line in f:; normalise with line.strip() and split via line.split('|', 1). For simple non-repeating selection, build an index list idx = list(range(n)), call random.shuffle(idx), and slice idx[:k].”"
      ],
      "metadata": {
        "id": "Cmfgfm9-nqBr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**My Manual Implementation:**"
      ],
      "metadata": {
        "id": "Dr_rUGcCrxCx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def load_quiz_questions(file_path):\n",
        "    \"\"\"\n",
        "    Load questions from file - basic file handling.\n",
        "    Assumes each line is 'Question text|Answer text'.\n",
        "\n",
        "    >>> # Structure-only doctest (I/O skipped in CI):\n",
        "    >>> # qs = load_quiz_questions(\"sample_quiz.txt\")  # doctest: +SKIP\n",
        "    >>> # isinstance(qs, list) and all(isinstance(x, tuple) and len(x) == 2 for x in qs)  # doctest: +SKIP\n",
        "    True\n",
        "    \"\"\"\n",
        "    # Week 6: file I/O + simple validation; Week 2: string handling; result is a list of (q, a) tuples\n",
        "    questions = []\n",
        "    try:\n",
        "        with open(file_path, \"r\") as f:\n",
        "            for raw in f:\n",
        "                line = raw.strip()\n",
        "                # Skip empty or malformed\n",
        "                if not line or \"|\" not in line:\n",
        "                    continue\n",
        "                q, a = line.split(\"|\", 1)\n",
        "                q, a = q.strip(), a.strip()\n",
        "                if q and a:  # keep only well-formed pairs\n",
        "                    questions.append((q, a))\n",
        "        # No advanced libraries; keep messages beginner-friendly\n",
        "        if not questions:\n",
        "            print(\"Warning: No valid question lines were found.\")\n",
        "    except FileNotFoundError:\n",
        "        print(\"Error: File not found. Please check the path.\")\n",
        "    except Exception as e:\n",
        "        # Keep errors readable for beginners\n",
        "        print(f\"Error: Unable to read the file ({e}).\")\n",
        "    return questions\n",
        "\n",
        "\n",
        "def select_random_questions(questions, num_questions):\n",
        "    \"\"\"\n",
        "    Select N random questions - basic random selection (no duplicates).\n",
        "\n",
        "    >>> sample = [(\"Q1\",\"A1\"),(\"Q2\",\"A2\"),(\"Q3\",\"A3\")]\n",
        "    >>> picked = select_random_questions(sample, 2)\n",
        "    >>> len(picked)\n",
        "    2\n",
        "    >>> isinstance(picked, list)\n",
        "    True\n",
        "    \"\"\"\n",
        "    # Week 7: simple non-repeating selection by shuffling indices\n",
        "    if not isinstance(questions, list) or len(questions) == 0:\n",
        "        return []\n",
        "    if not isinstance(num_questions, int) or num_questions <= 0:\n",
        "        return []\n",
        "\n",
        "    # Clamp to available size\n",
        "    total = len(questions)\n",
        "    n = num_questions if num_questions <= total else total\n",
        "\n",
        "    # Shuffle indices so we don't mutate the original list\n",
        "    idx = list(range(total))\n",
        "    random.shuffle(idx)\n",
        "    chosen = idx[:n]\n",
        "\n",
        "    # Build the result by index to avoid side effects\n",
        "    selected = []\n",
        "    for i in chosen:\n",
        "        selected.append(questions[i])\n",
        "    return selected\n",
        "\n",
        "\n",
        "def run_quiz_session(selected_questions):\n",
        "    \"\"\"\n",
        "    Conduct quiz and return score - basic loops and user interaction.\n",
        "\n",
        "    >>> # Interactive function (manual run in Colab/terminal):\n",
        "    >>> demo = [(\"2+2?\", \"4\")]\n",
        "    >>> # run_quiz_session(demo)  # doctest: +SKIP\n",
        "    \"\"\"\n",
        "    # Week 5: loop/condition; Week 6: simple input validation; Week 2: string normalisation\n",
        "    if not isinstance(selected_questions, list) or len(selected_questions) == 0:\n",
        "        print(\"No questions to run.\")\n",
        "        return 0\n",
        "\n",
        "    score = 0\n",
        "    total = len(selected_questions)\n",
        "\n",
        "    for q, a in selected_questions:\n",
        "        print(f\"Question: {q}\")\n",
        "        ans = input(\"Your answer: \").strip()\n",
        "        # Case-insensitive comparison; keep logic transparent\n",
        "        if ans.lower() == a.strip().lower():\n",
        "            print(\"Correct!\\n\")\n",
        "            score += 1\n",
        "        else:\n",
        "            print(f\"Incorrect. The correct answer is: {a}\\n\")\n",
        "\n",
        "    print(f\"You scored {score} out of {total}.\")\n",
        "    return score"
      ],
      "metadata": {
        "id": "hFR9_PklruK8"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Course Reference:**\n",
        "I implemented these functions by combining Week 2 (Lists and Strings) for storing (question, answer) pairs and normalising input, Week 5 (Iteration and Decision Making) for looping through items and comparing answers, Week 6 (File I/O and Basic Validation) for safe file reading with beginner-friendly error messages, and Week 7 (Simple Random Selection) for non-repeating sampling via shuffled indices. Each choice reflects our Section 1 pseudocode and the course’s AI-partnership guidance: AI provided only syntax reminders, while I authored the control logic manually. The overall solution remains strictly within the ≤ Week 10 scope and mirrors the straightforward, functional style used in our weekly exercises."
      ],
      "metadata": {
        "id": "GwwiTWMpr5EG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4.2 Course-Connected Reflection Practice (10 marks)**\n",
        "\n",
        "**Course Constraint Application：**\n",
        "I followed the Artificial Intelligence Partnership Guidelines by restricting the model’s suggestions to topics taught before Week Ten. When the artificial intelligence recommended advanced functions or external libraries, I simplified its ideas using the methods from Week Six File Operations and Validation and Week Five Loops and Conditionals. For instance, in the function, I replaced complex syntax with the basic statement and simple and handling to keep the program consistent with beginner-level practice.load_quiz_questionswith open()tryexcept\n",
        "\n",
        "**Learning Connection：**\n",
        "This exercise strengthened my understanding of how each weekly topic connects to practical programming. From Week Six File Handling and Input Checking, I applied validation techniques to ensure the quiz file existed and followed the “Question|Answer” format. The concept of preventing runtime errors through input validation and user-friendly error messages was especially helpful. It reminded me how clear, defensive programming supports accurate results and smoother user interaction.\n",
        "\n",
        "**Skill Development：**\n",
        "Before the final examination, I plan to deepen my knowledge of Week Eight Exception Handling Fundamentals by writing clearer and more specific error messages for different runtime situations. I will also improve my doctest writing from Week Four Function Design to include edge cases such as empty files or invalid question lines. These steps will help me produce more reliable and maintainable programs in future assessments."
      ],
      "metadata": {
        "id": "t0TKY9rrv80r"
      }
    }
  ]
}